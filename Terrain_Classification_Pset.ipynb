{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terrain Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-Time Setup:** *Run the code snippet below once. This sets up your file directory and unzips files. After running for the first time, comment out the code so it isn't run again*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile as zf\n",
    "files = zf.ZipFile(\"images.zip\", 'r')\n",
    "files.extractall('training/images/')\n",
    "files.close()\n",
    "files = zf.ZipFile(\"ground_truth.zip\", 'r')\n",
    "files.extractall('training/gt/')\n",
    "files.close()\n",
    "files = zf.ZipFile(\"test_images.zip\", 'r')\n",
    "files.extractall('testing/images/')\n",
    "files.close()\n",
    "files = zf.ZipFile(\"test_gt.zip\", 'r')\n",
    "files.extractall('testing/gt/')\n",
    "files.close()\n",
    "!cat x* > weights_A.hdf5\n",
    "\n",
    "!mkdir training/weights\n",
    "!mkdir prediction\n",
    "!mkdir prediction/usertrained\n",
    "!mkdir prediction/pretrained\n",
    "!mkdir prediction/usertrained/threshold\n",
    "!mkdir prediction/pretrained/threshold\n",
    "!mv weights_A.hdf5 training/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crater Segmentation from Aerial Images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to walk through the process of implementing, training, and testing out a convolutional neural network for crater segmentation. We will train the neural net on existing and labeled training images. In this case, we'll be working with a dataset of lunar images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The U-net is a CNN designed specificly for image segmentation. It was developed at the Universtiy of Freiburg, and published in 2015. \n",
    "\n",
    "As mentioned in the lecture, the general idea of it is to use a contracting path to compress the image and find features, and then use information from different layers in the network to precisely localize important information. \n",
    "You can see how the U-net code is implemented in the Unet.py file. \n",
    "\n",
    "If you are curious about more information on how the U-net works, we recommend reading the original paper: https://arxiv.org/pdf/1505.04597.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms require to data from which to learn. The quantity and quality of the dataset can have significant effects on the performance. Let's take a look at some of the images we will be learning from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![training_image_1](img_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The darker regions in the photo are created by shadowing where there are craters..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![training_image_0](img_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some images, craters may not be present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![training_image_2](img_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the craters may be challenging to pick out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in order to train the neural net, we need to have labeled \"truth\" to teach the neural net what's correct. An example is shown below. This \"truth\" image corresponds to the lunar image shown above. We will now train the unet on a set of labeled data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![truth_image_2](gt_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by installing some necessary packages, building out the file directory, and importing those packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install packages\n",
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "!pip install helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "ZYvX2Zeffyqy",
    "outputId": "d1cc4fd5-90aa-4572-b0f9-f0776c76c9ce"
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "from unet import *\n",
    "from augmentation import *\n",
    "from test_methods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1S0RNFV2m78I"
   },
   "source": [
    "### We start by initializing a convolutional neural network based on the U-net architecture. \n",
    "\n",
    "The summary shows how the layers are structured with input-shape, functions and number of neurons. You can see that the input has the same shape as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1686
    },
    "colab_type": "code",
    "id": "5f5nflRpfyq5",
    "outputId": "9df08f9a-4fa2-4604-b848-f8116456e7c9"
   },
   "outputs": [],
   "source": [
    "#create the cnn\n",
    "model = get_unet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvCDv06qupTg"
   },
   "source": [
    "#### We now declare our variables for training the network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by training with 10 steps per epoch, and 2 epochs. This is quite short, but as you'll see, we are quite limited computationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCOpsKYbfyq9"
   },
   "outputs": [],
   "source": [
    "#variables for how random images will be generated when training\n",
    "generator_variables = dict(rotation_range=90,\n",
    "                           width_shift_range=0.0,\n",
    "                           height_shift_range=0.0,\n",
    "                           zoom_range=0.7,\n",
    "                           horizontal_flip=True,\n",
    "                           vertical_flip = True,\n",
    "                           fill_mode='reflect')\n",
    "\n",
    "#number of images per gradient-calculation\n",
    "batch_size = 2\n",
    "#number of times we try gradients per epoch\n",
    "steps_per_epoch = 10\n",
    "epochs= 2\n",
    "#epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZUdsFQgnvVp6"
   },
   "source": [
    "#### Specify where the training images are stored, and where the model should be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MCcUj8h6fyq_"
   },
   "outputs": [],
   "source": [
    "#fetch images\n",
    "cloud_dir = os.getcwd() \n",
    "cloud_dir = cloud_dir + \"/\"\n",
    "train_dir = cloud_dir+\"training/\"\n",
    "#directory where checkpoint weights will be stored\n",
    "weights_dir = train_dir+\"weights/\"\n",
    "weights = weights_dir+'weights_B.hdf5'\n",
    "img_folder = 'images'\n",
    "gtruth_folder = 'gt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5yyqoBMSwHhO"
   },
   "source": [
    "#### Train the model using the generator-variables we previously specified\n",
    "By using checkpoint the model will be saved in the specified folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may take some time. Probably on the order of about 5 minutes per epoch. Go grab a snack and come back. (Or modify the training parameters above to do just one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "woDkZgHcfyrC",
    "outputId": "1a7b07c2-e33a-432a-ce4b-4076a7c5947f"
   },
   "outputs": [],
   "source": [
    "#we make a generator that will be used for training\n",
    "train_generator = trainGenerator(batch_size,train_dir,img_folder,gtruth_folder,generator_variables)\n",
    "#we save the weights to a file\n",
    "checkpoint = ModelCheckpoint(weights, monitor='loss', verbose=1, save_best_only=True)\n",
    "#we train the model using the generator\n",
    "model.fit_generator(train_generator,steps_per_epoch=steps_per_epoch,epochs=epochs, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H_7fFOvYwhqW"
   },
   "source": [
    "#### We now have a trained model that can be used to perform image segmentation of craters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that it takes a lot of time to train the neural net on a regular CPU. As a result, we'll give you a pre-trained model that has been trained for 600 steps per epoch, and 10 epochs. \n",
    "\n",
    "(We pre-trained the model for you using Google Colab's GPU. Feel free to try it out yourself!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model performs by giving it data that it has never seen before. This is a different set of images than the training set, since using new images is critical for evaluating whether our model extrapolates well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We declare the name of the file containing the pretrained weights, and where to save the predicted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = os.getcwd() + '/training/weights/' + 'weights_A.hdf5' # <- this is the pre-trained model\n",
    "model.load_weights(weights)\n",
    "save_imgs = True\n",
    "where_to_save = \"prediction/pretrained/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use generator to make predictions for all images\n",
    "By using the generator we avoid loading all images to the memory at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The generator is created with the name of the folder for the test-images as its parameter\n",
    "testGene = testGenerator(\"testing/images\")\n",
    "#run the prediction with the generator, and declare how many images to include.\n",
    "results = model.predict_generator(testGene,10,verbose=1)\n",
    "if(save_imgs):\n",
    "    save_result(where_to_save,results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see the output of one of the prediction images. The neural net outputs a probability for each pixel, based on how confident it is that each pixel is part of a crater edge. In some applications, we may want to have a concrete classification of what is predicted to be a crater edge, and what is not. (In other applications, we may prefer a likelihood.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'prediction/pretrained/1_predict.png'\n",
    "image = Image.open(fname)#.convert(\"L\")\n",
    "arr = np.asarray(image)\n",
    "plt.imshow(arr, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now consider what should count as a crater. Let's define a threshold--the value that each pixel has to exceed in order to be considered a crater edge. (We'll normalize it to a value between 0 and 1.) And let's look at how changing that threshold will affect the results.\n",
    "\n",
    "The *threshold_prediction* function takes in an image (as a PixelAccess object) and a specified threshold value (from 0 to 1) and outputs the image with each pixel classified as a crater boundary in white, and everything else in black. Remember that the threshold value is normalized to between 0 and 1, but the PixelAccess object contains pixel values between 0 (black) and 65535 (white). Access a pixel with coordinates (i,j) in an image *pix* by pix[i,j]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e2bcfa6ee00016be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def threshold_prediction(pix,thres):\n",
    "    \n",
    "    image_size = im.size # Get width and height of image\n",
    "\n",
    "    WHITE = 65535 #pixel value for white\n",
    "    BLACK = 0     #pixel value for black\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "\n",
    "    value_thres = thres * WHITE #threshold value as a percentage of the white value required to classify as part of crater edge\n",
    "\n",
    "    for i in range(image_size[0]):\n",
    "        for j in range(image_size[1]):\n",
    "            if pix[i,j] > value_thres:\n",
    "                pix[i,j] = WHITE\n",
    "            else:\n",
    "                pix[i,j] = BLACK\n",
    "\n",
    "    return pix\n",
    "\n",
    "    ### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bb720d0d1acbe3c8",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that accuracy functions work correctly\"\"\"\n",
    "from nose.tools import assert_equal\n",
    "from utils import test_ok\n",
    "\n",
    "thresholds = [.25,.1]\n",
    "expected_pix_25 = [[0, 65535, 65535, 0],\n",
    " [0, 0, 65535, 65535],\n",
    " [0, 0, 65535, 65535],\n",
    " [0, 0, 0, 65535],\n",
    " [0, 0, 0, 0]]\n",
    "expected_pix_10 = [[0, 65535, 65535, 65535],\n",
    " [0, 65535, 65535, 65535],\n",
    " [0, 0, 65535, 65535],\n",
    " [0, 0, 0, 65535],\n",
    " [0, 0, 0, 0]]\n",
    "expected_pixs = [expected_pix_25, expected_pix_10]\n",
    "for i in range(2):\n",
    "    im = Image.open('prediction/pretrained/8_predict.png')\n",
    "    im = im.crop((30, 34, 35, 38))\n",
    "    pix = im.load()\n",
    "    pix = threshold_prediction(pix,thresholds[i])\n",
    "    x = expected_pixs[i]\n",
    "    for i in range(im.size[0]):\n",
    "        for j in range(im.size[1]):\n",
    "            assert_equal(x[i][j], pix[i, j])\n",
    "            x[i][j] = pix[i, j]\n",
    "\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [.25,.1]\n",
    "for i in range(2):\n",
    "    im = Image.open('prediction/pretrained/1_predict.png') # Can be many different formats.\n",
    "    #im = Image.open('test_truth/test_gt_1.png') # Can be many different formats.\n",
    "    pix = im.load() #PixelAccess object\n",
    "    pix = threshold_prediction(pix,thresholds[i])\n",
    "    im.save('prediction/pretrained/threshold/1_predict_' + str(i) + '.png')  # Save the modified pixels as .png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below for the image thresholded at .25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#![Thresholded_pred](prediction/pretrained/threshold/1_predict_0.png)\n",
    "fname = 'prediction/pretrained/threshold/1_predict_0.png'\n",
    "image = Image.open(fname)\n",
    "arr = np.asarray(image)\n",
    "plt.imshow(arr, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this next image is for a threshold of .1. Notice the presence of more craters, but also thicker edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#![Thresholded_pred](prediction/pretrained/threshold/1_predict_1.png)\n",
    "fname = 'prediction/pretrained/threshold/1_predict_1.png'\n",
    "image = Image.open(fname)\n",
    "arr = np.asarray(image)\n",
    "plt.imshow(arr, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare it with the \"right answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#![Thresholded_pred](testing/gt/test_gt_1.png)\n",
    "fname = 'testing/gt/test_gt_1.png'\n",
    "image = Image.open(fname)\n",
    "arr = np.asarray(image)\n",
    "plt.imshow(arr, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,3)\n",
    "f.set_figwidth(15)\n",
    "\n",
    "fname = 'prediction/pretrained/threshold/1_predict_0.png'\n",
    "image = Image.open(fname)\n",
    "arr = np.asarray(image)\n",
    "axarr[0].set_title('threshold .25')\n",
    "axarr[0].imshow(arr, cmap='gray')\n",
    "\n",
    "fname = 'prediction/pretrained/threshold/1_predict_1.png'\n",
    "image = Image.open(fname)\n",
    "arr = np.asarray(image)\n",
    "axarr[1].set_title('threshold .1')\n",
    "axarr[1].imshow(arr, cmap='gray')\n",
    "\n",
    "fname = 'testing/gt/test_gt_1.png'\n",
    "image = Image.open(fname)\n",
    "arr = np.asarray(image)\n",
    "axarr[2].set_title('Truth')\n",
    "axarr[2].imshow(arr, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conceptual Question:** Why might 50% confidence not be the right choice? For a Mars mission, would you want to choose a lower or higher threshold? Why? (Hint: Consider the consequences to Mark Watney based on these classifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a8b1ec1de7d825b3",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "    ### BEGIN SOLUTION\n",
    "    A: In a planetary exploration mission, risk avoidance is far more important than finding the absolute shortest path.  Therefore, even a 1% chance of an area containing a potentially rover-damaging crater might be too high for us to be willing to drive through that area. We are more inclined to choose a lower threshold for safety critical functions.\n",
    "\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics\n",
    "We now want to evaluate the accuracy of the classification at different thresholds. To do that, we need a metric by which to evaluate. Let us consider three different metrics. \n",
    "\n",
    "*Overall accuracy* looks at every pixel in an image and asks whether each is classified correctly, as either part of a *crater edge* or *not crater edges*. \n",
    "\n",
    "The *true positive rate* is only interested in whether the true craters were detected. In other words, it looks only at the pixels which correspond to *crater edges* in the actual truth data (white pixels), and determines whether the prediction was able to successfully classify those pixels as crater edges.\n",
    "\n",
    "The *true negative rate* is only interested in areas that are *not crater edges*. This looks only at the black pixels in the truth image, and asks whether each of those non-crater pixels were classified correctly by the prediction image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write three functions to evaluate the percentage of pixels classified correctly based on those three metrics. All three functions will take input arguments of:\n",
    "- a PixelAccess object (see above code for example usage) representing the prediction image, where crater edges are represented by white pixels (value = 65535)\n",
    "- a second PixelAccess object representing the actual truth data, \n",
    "- the size of that PixelAccess object (width and height in pixels). \n",
    "\n",
    "All three functions will return a percentage (from 0 to 1):\n",
    "- The first function, *percentcorrect_all* , will calculate the percentage of correct pixels over the entire image--number of black and white pixels classified correctly. \n",
    "- The second function, *percentcorrect_craters* , will calculate the percentage of crater pixels (based on the truth data) that were classified correctly--only looking at the white pixels from the ground truth image and whether the prediction classified those crater pixels correctly. This is the true positive rate. **In images where there are no craters in the truth data, and therefore no white pixels, the % correct of white pixels should return the float NaN**\n",
    "- The third function, *percentcorrect_noncraters* , will be very similar to the second function, except that you want to calculate the percentage of noncrater pixels that were classified correctly. In other words, this is the true negative rate.\n",
    "\n",
    "Make sure that the most recent predictions were run with the pre-trained model that we provided to you, as the autograder is expecting the pre-trained model to have been the most recent one used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c874a2a767122fe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def percentcorrect_all(pix_pred, pix_truth, image_size):\n",
    "    \n",
    "    WHITE = 65535 #pixel value for white\n",
    "    BLACK = 0     #pixel value for black\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    num_correct_pixels = 0\n",
    "    for i in range(image_size[0]):\n",
    "        for j in range(image_size[1]):\n",
    "            if pix_pred[i,j] == pix_truth[i,j]:\n",
    "                num_correct_pixels += 1\n",
    "    return num_correct_pixels/(image_size[0]*image_size[1])\n",
    "\n",
    "    ### END SOLUTION\n",
    "    \n",
    "\n",
    "    \n",
    "def percentcorrect_craters(pix_pred, pix_truth, image_size):\n",
    "    \n",
    "    WHITE = 65535 #pixel value for white\n",
    "    BLACK = 0     #pixel value for black\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    num_correct_pixels = 0\n",
    "    num_white_pixels = 0\n",
    "    for i in range(image_size[0]):\n",
    "        for j in range(image_size[1]):\n",
    "            if pix_truth[i,j] == WHITE:\n",
    "                if pix_pred[i,j] == pix_truth[i,j]:\n",
    "                    num_correct_pixels += 1\n",
    "                num_white_pixels += 1\n",
    "    if num_white_pixels != 0:\n",
    "        return num_correct_pixels/num_white_pixels\n",
    "    else:\n",
    "        return float('NaN')\n",
    "    \n",
    "    ### END SOLUTION\n",
    "\n",
    "def percentcorrect_noncraters(pix_pred, pix_truth, image_size):\n",
    "    \n",
    "    WHITE = 65535 #pixel value for white\n",
    "    BLACK = 0     #pixel value for black\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    num_correct_pixels = 0\n",
    "    num_black_pixels = 0\n",
    "    for i in range(image_size[0]):\n",
    "        for j in range(image_size[1]):\n",
    "            if pix_truth[i,j] == BLACK:\n",
    "                if pix_pred[i,j] == pix_truth[i,j]:\n",
    "                    num_correct_pixels += 1\n",
    "                num_black_pixels += 1\n",
    "                \n",
    "    if num_black_pixels != 0:\n",
    "        return num_correct_pixels/num_black_pixels\n",
    "    else:\n",
    "        return float('NaN')\n",
    "    \n",
    "    ### END SOLUTION\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code to make sure it's working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a25fb38d515e9961",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that accuracy functions work correctly\"\"\"\n",
    "from nose.tools import assert_almost_equal, assert_true\n",
    "import math\n",
    "from utils import test_ok\n",
    "\n",
    "#Test normal image\n",
    "im = Image.open('prediction/pretrained/' + str(1) +'_predict.png') # Can be many different formats.\n",
    "im_gt = Image.open('testing/gt/test_gt_' + str(1) + '.png') # Can be many different formats.\n",
    "pix = im.load()\n",
    "pix_gt = im_gt.load()\n",
    "image_size = im.size # Get width and height of image\n",
    "\n",
    "pix = threshold_prediction(pix,.25)\n",
    "\n",
    "acc_all = percentcorrect_all(pix, pix_gt, image_size)\n",
    "acc_craters = percentcorrect_craters(pix, pix_gt, image_size)\n",
    "acc_noncraters = percentcorrect_noncraters(pix, pix_gt, image_size)\n",
    "\n",
    "assert_almost_equal(acc_all, 0.871551513671875, delta=0.001)\n",
    "assert_almost_equal(acc_craters, 0.4389568764568765, delta=0.001)\n",
    "assert_almost_equal(acc_noncraters, 0.9221604854104173, delta=0.001)\n",
    "\n",
    "#Test image with no craters\n",
    "im = Image.open('prediction/pretrained/' + str(0) +'_predict.png') # Can be many different formats.\n",
    "im_gt = Image.open('testing/gt/test_gt_' + str(0) + '.png') # Can be many different formats.\n",
    "pix = im.load()\n",
    "pix_gt = im_gt.load()\n",
    "image_size = im.size # Get width and height of image\n",
    "\n",
    "pix = threshold_prediction(pix,.25)\n",
    "\n",
    "acc_all = percentcorrect_all(pix, pix_gt, image_size)\n",
    "acc_craters = percentcorrect_craters(pix, pix_gt, image_size)\n",
    "acc_noncraters = percentcorrect_noncraters(pix, pix_gt, image_size)\n",
    "\n",
    "assert_almost_equal(acc_all, 1.0, delta=0.001)\n",
    "assert_true(math.isnan(acc_craters))\n",
    "assert_almost_equal(acc_noncraters, 1.0, delta=0.001)\n",
    "\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine different thresholds\n",
    "We will now examine a range of different thresholds and how the accuracy varies for each metric, with 5 of the images we predicted. Feel free to try more (you may need to predict more images first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [.01, .02, .03, .05, .075, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1] #thresholds to try\n",
    "n_images = 5 # number of images (already predicted) to evaluate\n",
    "acc_all = np.zeros([len(thresholds),n_images])\n",
    "acc_craters = np.zeros([len(thresholds),n_images])\n",
    "acc_noncraters = np.zeros([len(thresholds),n_images])\n",
    "mean_acc_all = np.zeros(len(thresholds))\n",
    "mean_acc_craters = np.zeros(len(thresholds))\n",
    "mean_acc_noncraters = np.zeros(len(thresholds))\n",
    "\n",
    "for i in range(len(thresholds)):\n",
    "    for j in range(n_images):\n",
    "        im = Image.open('prediction/pretrained/' + str(j) +'_predict.png') # Can be many different formats.\n",
    "        im_gt = Image.open('testing/gt/test_gt_' + str(j) + '.png') # Can be many different formats.\n",
    "        pix = im.load()\n",
    "        pix_gt = im_gt.load()\n",
    "        image_size = im.size # Get width and height of image\n",
    "\n",
    "        pix = threshold_prediction(pix,thresholds[i])\n",
    "        acc_all[i][j] = percentcorrect_all(pix, pix_gt, image_size)\n",
    "        acc_craters[i][j] = percentcorrect_craters(pix, pix_gt, image_size)\n",
    "        acc_noncraters[i][j] = percentcorrect_noncraters(pix, pix_gt, image_size)\n",
    "        \n",
    "    mean_acc_all[i] = np.nanmean(acc_all[i])\n",
    "    mean_acc_craters[i] = np.nanmean(acc_craters[i])\n",
    "    mean_acc_noncraters[i] = np.nanmean(acc_noncraters[i])\n",
    "    \n",
    "\n",
    "f, axarr = plt.subplots(1,2)\n",
    "f.set_figwidth(15)\n",
    "\n",
    "axarr[0].plot(thresholds,mean_acc_all,'r')\n",
    "axarr[0].plot(thresholds,mean_acc_craters,'c')\n",
    "axarr[0].plot(thresholds,mean_acc_noncraters,'m')\n",
    "axarr[0].set_ylabel('Accuracy')\n",
    "axarr[0].set_xlabel('Threshold')\n",
    "axarr[0].legend(['Overall','Craters','NonCraters'])\n",
    "axarr[0].set_title('Detecting Craters: Accuracy of Different Methods')\n",
    "\n",
    "axarr[1].plot(thresholds,1-mean_acc_noncraters)\n",
    "axarr[1].plot(thresholds,1-mean_acc_craters)\n",
    "axarr[1].set_ylabel('Missed Detection Rate')\n",
    "axarr[1].set_xlabel('Threshold')\n",
    "axarr[1].legend(['Missed Craters','Missed NonCraters'])\n",
    "axarr[1].set_title('Missed Detections: Accuracy of Different Methods')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left plot, we look at the accuracy of detection using the three metrics we defined above. The red line is the overall accuracy (rate of correct detection for both crater and noncrater pixels). The cyan line shows the rate of detection of craters, and the magenta line shows the rate of detection of noncraters (areas that are safe).\n",
    "\n",
    "However, we can also look at the same information from a different perspective--that of missed detections. In the right plot, we look at the rate of missed detections for craters, and the rate of misclassification of noncrater (safe) regions. The rate of missed detection is simply (1 - the rate of detection).\n",
    "\n",
    "#### Receiver Operating Characteristic Curve\n",
    "\n",
    "Another way to view this is the Receiver Operating Characteristic (ROC) curve, shown below. The ROC curve plots the rate of positive detection against the rate of false positives. This allows a direct comparison of the tradeoffs between the rate of true crater detections and false alarms. We'll also plot a similar one for missed detections by comparing false negatives to false positives. This compares the rate of missed craters with the rate of missed safe regions. Both plots essentially tell the same story, but from different perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1,2)\n",
    "f.set_figwidth(15)\n",
    "\n",
    "axarr[0].plot(1-mean_acc_noncraters,mean_acc_craters)\n",
    "axarr[0].set_xlabel('False Positive Rate')\n",
    "axarr[0].set_ylabel('True Positive Rate')\n",
    "axarr[0].set_title('ROC Curve: Crater Detection vs. False Alarms')\n",
    "\n",
    "\n",
    "axarr[1].plot(1-mean_acc_noncraters,1-mean_acc_craters)\n",
    "axarr[1].set_xlabel('False Positive Rate')\n",
    "axarr[1].set_ylabel('False Negative Rate')\n",
    "axarr[1].set_title('ROC Curve: Missed Detection vs. Missed Safe Regions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conceptual Question:** Explain the trends that you see in the above plots. How would you select the threshold to be used? Why might using the overall accuracy (across black and white pixels) to determine which threshold to use be a good or bad idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-76b887688157ac40",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "    ### BEGIN SOLUTION\n",
    "    As the threshold decreases, the percentage of craters detected increases, but accuracy of non-crater regions decreases. This illustrates the tradeoff between being conservative and classifying non-crater regions accurately. The ROC curve shows that there will always be a tradeoff. A higher rate of crater detection means a lower rate of noncrater detection. One method of selecting the threshold may be to find the \"knee\" in the graph, where the derivative are equal. Another method would be to set a requirement for crater accuracy, and adjusting the threshold to meet that. [Answers may vary.] However, using the overall accuracy is not a good metric, as most of the pixels are black, so a naive guess that everything is not a crater may appear to have high overall accuracy, but result in catastrophic consequences when a rover crashes into a crater.\n",
    "\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Terrain Classification Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to terrain classification using aerial images, the lecture also discussed using images taken by the rover and using vibration and velocity signals from the rover's physical interaction with the terrain to do terrain classification.  The lecture also touched on combining these lower-level classifiers into a more accurate meta-classifier.  These final few conceptual questions will highlight some of the key takeaways from these other techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground-Based Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discussed a few machine learning techniques that use color, texture, and geometry to differentiate between terrain types like sand and rock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conceptual Question:** Why is color alone not a robust enough visual classifier on Mars?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-10d4c83bcdf7ba53",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "    ### BEGIN SOLUTION\n",
    "    A:  Illumination can change r,g,b values so training on one light setting could cause the classifier to fail in different lighting conditions.  Additionally there is very little color variation on Mars.\n",
    "\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A geometric hazard refers to something like a rock that has a defined geometric shape.  The rover tries to avoid large rocks because, obviously, running into a rock could be hazardous to the rover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conceptual Question:** What is an example of a non-geometric hazard and why might detection of these types of hazards be beneficial to detect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4d25944b2d82c827",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "    ### BEGIN SOLUTION\n",
    "    An example could be loose sand on a steep slope; we want to detect this because this could lead to significant slipping and the rover could get stuck\n",
    "\n",
    "\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tactile Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we discussed how to process vibration and velocity signals from the rover's wheels to be able to determine the type of ground the rover is driving over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conceptual Question:** When might tactile-based classification be beneficial over image-based classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ac96487c020cc9c1",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "    ### BEGIN SOLUTION\n",
    "    A: When situations not represented in the training set are encountered. When lighting is poor or vision is obscured for some reason (i.e. a dust storm), or you need to classify the ground directly beneath the rover instead of in front of the rover for some reason\n",
    "\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conceptual Question:** In what situations might tactile-based classification fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-134eff3b197bf573",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "    ### BEGIN SOLUTION\n",
    "    A: When situations not represented in the training set are encountered. For tactile-based classification, this could mean the rover is traversing terrain that was not examined fully in the training set or it's traveling at a speed or with a load for which data was not gathered in the training set.\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last section of the lecture discussed combining the earlier classifiers to build a stronger classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conceptual Question:** Describe at least one benefit of using ground-based image classification and tactile classification in conjunction with one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-637529dad510c08c",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "    ### BEGIN SOLUTION\n",
    "    Different methods may have higher accuracies on different terrain or in different conditions, and using multiple methods may provide for taking advantage of each classifier where they are more accurate. Additionally, the tactile classification methods may provide labels and truth data for training visual classifiers, allowing the rover to learn new terrain from experience without human supervision.\n",
    "\n",
    "    ### END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Create Assignment",
  "colab": {
   "name": "train_craters.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
